{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "caltech256.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWd35Grmgr08",
        "colab_type": "code",
        "outputId": "904cc519-ae8a-491f-d131-2f18f8cad91b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQZhvGECiJXz",
        "colab_type": "code",
        "outputId": "62b01ff2-9ac0-4a9e-cc84-d0f4766e2e05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "!pip install scipy==1.1.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scipy==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.17.4)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "  Found existing installation: scipy 1.3.2\n",
            "    Uninstalling scipy-1.3.2:\n",
            "      Successfully uninstalled scipy-1.3.2\n",
            "Successfully installed scipy-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtjKVWFhg2Ar",
        "colab_type": "code",
        "outputId": "ffc1f915-5298-4bcf-9eb4-78b16ee0b2f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!ls drive/'My Drive'/256_ObjectCategories\t"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "001.ak47\t\t 087.goldfish\t      173.rifle\n",
            "002.american-flag\t 088.golf-ball\t      174.rotary-phone\n",
            "003.backpack\t\t 089.goose\t      175.roulette-wheel\n",
            "004.baseball-bat\t 090.gorilla\t      176.saddle\n",
            "005.baseball-glove\t 091.grand-piano-101  177.saturn\n",
            "006.basketball-hoop\t 092.grapes\t      178.school-bus\n",
            "007.bat\t\t\t 093.grasshopper      179.scorpion-101\n",
            "008.bathtub\t\t 094.guitar-pick      180.screwdriver\n",
            "009.bear\t\t 095.hamburger\t      181.segway\n",
            "010.beer-mug\t\t 096.hammock\t      182.self-propelled-lawn-mower\n",
            "011.billiards\t\t 097.harmonica\t      183.sextant\n",
            "012.binoculars\t\t 098.harp\t      184.sheet-music\n",
            "013.birdbath\t\t 099.harpsichord      185.skateboard\n",
            "014.blimp\t\t 100.hawksbill-101    186.skunk\n",
            "015.bonsai-101\t\t 101.head-phones      187.skyscraper\n",
            "016.boom-box\t\t 102.helicopter-101   188.smokestack\n",
            "017.bowling-ball\t 103.hibiscus\t      189.snail\n",
            "018.bowling-pin\t\t 104.homer-simpson    190.snake\n",
            "019.boxing-glove\t 105.horse\t      191.sneaker\n",
            "020.brain-101\t\t 106.horseshoe-crab   192.snowmobile\n",
            "021.breadmaker\t\t 107.hot-air-balloon  193.soccer-ball\n",
            "022.buddha-101\t\t 108.hot-dog\t      194.socks\n",
            "023.bulldozer\t\t 109.hot-tub\t      195.soda-can\n",
            "024.butterfly\t\t 110.hourglass\t      196.spaghetti\n",
            "025.cactus\t\t 111.house-fly\t      197.speed-boat\n",
            "026.cake\t\t 112.human-skeleton   198.spider\n",
            "027.calculator\t\t 113.hummingbird      199.spoon\n",
            "028.camel\t\t 114.ibis-101\t      200.stained-glass\n",
            "029.cannon\t\t 115.ice-cream-cone   201.starfish-101\n",
            "030.canoe\t\t 116.iguana\t      202.steering-wheel\n",
            "031.car-tire\t\t 117.ipod\t      203.stirrups\n",
            "032.cartman\t\t 118.iris\t      204.sunflower-101\n",
            "033.cd\t\t\t 119.jesus-christ     205.superman\n",
            "034.centipede\t\t 120.joy-stick\t      206.sushi\n",
            "035.cereal-box\t\t 121.kangaroo-101     207.swan\n",
            "036.chandelier-101\t 122.kayak\t      208.swiss-army-knife\n",
            "037.chess-board\t\t 123.ketch-101\t      209.sword\n",
            "038.chimp\t\t 124.killer-whale     210.syringe\n",
            "039.chopsticks\t\t 125.knife\t      211.tambourine\n",
            "040.cockroach\t\t 126.ladder\t      212.teapot\n",
            "041.coffee-mug\t\t 127.laptop-101       213.teddy-bear\n",
            "042.coffin\t\t 128.lathe\t      214.teepee\n",
            "043.coin\t\t 129.leopards-101     215.telephone-box\n",
            "044.comet\t\t 130.license-plate    216.tennis-ball\n",
            "045.computer-keyboard\t 131.lightbulb\t      217.tennis-court\n",
            "046.computer-monitor\t 132.light-house      218.tennis-racket\n",
            "047.computer-mouse\t 133.lightning\t      219.theodolite\n",
            "048.conch\t\t 134.llama-101\t      220.toaster\n",
            "049.cormorant\t\t 135.mailbox\t      221.tomato\n",
            "050.covered-wagon\t 136.mandolin\t      222.tombstone\n",
            "051.cowboy-hat\t\t 137.mars\t      223.top-hat\n",
            "052.crab-101\t\t 138.mattress\t      224.touring-bike\n",
            "053.desk-globe\t\t 139.megaphone\t      225.tower-pisa\n",
            "054.diamond-ring\t 140.menorah-101      226.traffic-light\n",
            "055.dice\t\t 141.microscope       227.treadmill\n",
            "056.dog\t\t\t 142.microwave\t      228.triceratops\n",
            "057.dolphin-101\t\t 143.minaret\t      229.tricycle\n",
            "058.doorknob\t\t 144.minotaur\t      230.trilobite-101\n",
            "059.drinking-straw\t 145.motorbikes-101   231.tripod\n",
            "060.duck\t\t 146.mountain-bike    232.t-shirt\n",
            "061.dumb-bell\t\t 147.mushroom\t      233.tuning-fork\n",
            "062.eiffel-tower\t 148.mussels\t      234.tweezer\n",
            "063.electric-guitar-101  149.necktie\t      235.umbrella-101\n",
            "064.elephant-101\t 150.octopus\t      236.unicorn\n",
            "065.elk\t\t\t 151.ostrich\t      237.vcr\n",
            "066.ewer-101\t\t 152.owl\t      238.video-projector\n",
            "067.eyeglasses\t\t 153.palm-pilot       239.washing-machine\n",
            "068.fern\t\t 154.palm-tree\t      240.watch-101\n",
            "069.fighter-jet\t\t 155.paperclip\t      241.waterfall\n",
            "070.fire-extinguisher\t 156.paper-shredder   242.watermelon\n",
            "071.fire-hydrant\t 157.pci-card\t      243.welding-mask\n",
            "072.fire-truck\t\t 158.penguin\t      244.wheelbarrow\n",
            "073.fireworks\t\t 159.people\t      245.windmill\n",
            "074.flashlight\t\t 160.pez-dispenser    246.wine-bottle\n",
            "075.floppy-disk\t\t 161.photocopier      247.xylophone\n",
            "076.football-helmet\t 162.picnic-table     248.yarmulke\n",
            "077.french-horn\t\t 163.playing-card     249.yo-yo\n",
            "078.fried-egg\t\t 164.porcupine\t      250.zebra\n",
            "079.frisbee\t\t 165.pram\t      251.airplanes-101\n",
            "080.frog\t\t 166.praying-mantis   252.car-side-101\n",
            "081.frying-pan\t\t 167.pyramid\t      253.faces-easy-101\n",
            "082.galaxy\t\t 168.raccoon\t      254.greyhound\n",
            "083.gas-pump\t\t 169.radio-telescope  255.tennis-shoes\n",
            "084.giraffe\t\t 170.rainbow\t      256.toad\n",
            "085.goat\t\t 171.refrigerator     257.clutter\n",
            "086.golden-gate-bridge\t 172.revolver-101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVkYaqtUgYK1",
        "colab_type": "code",
        "outputId": "3b59ffb9-97d7-4611-a9ab-e7b47077df8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.constraints import maxnorm\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import initializers\n",
        "from keras import backend as K\n",
        "# K.set_image_dim_ordering('th')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import scipy.misc\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "187NRz9MhJmi",
        "colab_type": "code",
        "outputId": "baa0906e-f046-4bea-dc00-cbbde80e0e15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        }
      },
      "source": [
        "def imread(path):\n",
        "    img = scipy.misc.imread(path).astype(np.float)\n",
        "    if len(img.shape) == 2:\n",
        "        img = np.transpose(np.array([img, img, img]), (2, 0, 1))\n",
        "    return img\n",
        "    \n",
        "#cwd = os.getcwd()\n",
        "path = \"drive/My Drive/256_ObjectCategories\"\n",
        "valid_exts = [\".jpg\", \".gif\", \".png\", \".peg\"]\n",
        "print (\"[%d] CATEGORIES ARE IN \\n %s\" % (len(os.listdir(path)), path))\n",
        "\n",
        "categories = sorted(os.listdir(path))\n",
        "ncategories = len(categories)\n",
        "imgs = []\n",
        "labels = []\n",
        "# LOAD ALL IMAGES \n",
        "for i, category in enumerate(categories):\n",
        "    iter = 0\n",
        "    for f in os.listdir(path + \"/\" + category):\n",
        "        if iter == 0:\n",
        "            ext = os.path.splitext(f)[1]\n",
        "            if ext.lower() not in valid_exts:\n",
        "                continue\n",
        "            fullpath = os.path.join(path + \"/\" + category, f)\n",
        "            img = scipy.misc.imresize(imread(fullpath), [128,128, 3])\n",
        "            img = img.astype('float32')\n",
        "            img[:,:,0] -= 123.68\n",
        "            img[:,:,1] -= 116.78\n",
        "            img[:,:,2] -= 103.94\n",
        "            imgs.append(img) # NORMALIZE IMAGE \n",
        "            label_curr = i\n",
        "            labels.append(label_curr)\n",
        "        #iter = (iter+1)%10;\n",
        "print (\"Num imgs: %d\" % (len(imgs)))\n",
        "print (\"Num labels: %d\" % (len(labels)) )\n",
        "print (ncategories)\n",
        "\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(imgs, labels, test_size = 0.7)\n",
        "X_train = np.stack(X_train, axis=0)\n",
        "y_train = np.stack(y_train, axis=0)\n",
        "X_test = np.stack(X_test, axis=0)\n",
        "y_test = np.stack(y_test, axis=0)\n",
        "print (\"Num train_imgs: %d\" % (len(X_train)))\n",
        "print (\"Num test_imgs: %d\" % (len(X_test)))\n",
        "# # one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes= y_test.shape[1]\n",
        "\n",
        "print(y_test.shape)\n",
        "print(X_train[1,1,1,:])\n",
        "print(y_train[1])\n",
        "# normalize inputs from 0-255 to 0.0-1.0\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "# X_train = X_train.transpose(0, 3, 1, 2)\n",
        "# X_test = X_test.transpose(0, 3, 1, 2)\n",
        "# print(X_train.shape)\n",
        "# print(X_test.shape)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[257] CATEGORIES ARE IN \n",
            " drive/My Drive/256_ObjectCategories\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Num imgs: 506\n",
            "Num labels: 506\n",
            "257\n",
            "Num train_imgs: 151\n",
            "Num test_imgs: 355\n",
            "(355, 257)\n",
            "[-3.6800003 12.220001  -5.9400024]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "(151, 128, 128, 3)\n",
            "(355, 128, 128, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9vpQ9aahOGR",
        "colab_type": "code",
        "outputId": "41912956-4db0-4594-ea72-f25334192a0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.regularizers import l1, l2\n",
        "from keras.callbacks import EarlyStopping\n",
        "#earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "# model.add(Conv2D(32, (3, 3), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "# model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), input_shape=(128, 128, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "# Compile mode\n",
        "epochs = 300\n",
        "lrate = 0.0001\n",
        "decay = lrate/epochs\n",
        "#sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
        "adam = SGD(lr=0.0001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "np.random.seed(seed)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 128, 128, 64)      1792      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 128, 128, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 64, 64, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 64, 64, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 32, 32, 256)       295168    \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 16, 16, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4096)              33558528  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 257)               1052929   \n",
            "=================================================================\n",
            "Total params: 71,417,153\n",
            "Trainable params: 71,417,153\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PtgQ54PhRiZ",
        "colab_type": "code",
        "outputId": "79eb474e-3c5d-4b3d-929e-50ab1f825a6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
        "          epochs=epochs, batch_size=56, shuffle=True)\n",
        "#hist = model.load_weights('./64.15/model.h5');\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.legend(['train','test'])\n",
        "plt.title('loss')\n",
        "plt.savefig(\"loss7.png\",dpi=300,format=\"png\")\n",
        "plt.figure()\n",
        "plt.plot(hist.history['acc'])\n",
        "plt.plot(hist.history['val_acc'])\n",
        "plt.legend(['train','test'])\n",
        "plt.title('accuracy')\n",
        "plt.savefig(\"accuracy7.png\",dpi=300,format=\"png\")\n",
        "model_json = model.to_json()\n",
        "with open(\"model7.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model7.h5\")\n",
        "print(\"Saved model to disk\")\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 151 samples, validate on 355 samples\n",
            "Epoch 1/300\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "151/151 [==============================] - 24s 156ms/step - loss: 5.6312 - acc: 0.0000e+00 - val_loss: 5.6025 - val_acc: 0.0000e+00\n",
            "Epoch 2/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.6235 - acc: 0.0000e+00 - val_loss: 5.5874 - val_acc: 0.0000e+00\n",
            "Epoch 3/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.6051 - acc: 0.0000e+00 - val_loss: 5.5732 - val_acc: 0.0000e+00\n",
            "Epoch 4/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.5818 - acc: 0.0000e+00 - val_loss: 5.5591 - val_acc: 0.0000e+00\n",
            "Epoch 5/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.5701 - acc: 0.0000e+00 - val_loss: 5.5454 - val_acc: 0.0000e+00\n",
            "Epoch 6/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.5574 - acc: 0.0000e+00 - val_loss: 5.5320 - val_acc: 0.0000e+00\n",
            "Epoch 7/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.5586 - acc: 0.0000e+00 - val_loss: 5.5188 - val_acc: 0.0000e+00\n",
            "Epoch 8/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.5425 - acc: 0.0000e+00 - val_loss: 5.5055 - val_acc: 0.0000e+00\n",
            "Epoch 9/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.5283 - acc: 0.0000e+00 - val_loss: 5.4921 - val_acc: 0.0000e+00\n",
            "Epoch 10/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.5290 - acc: 0.0000e+00 - val_loss: 5.4788 - val_acc: 0.0000e+00\n",
            "Epoch 11/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.5097 - acc: 0.0000e+00 - val_loss: 5.4655 - val_acc: 0.0000e+00\n",
            "Epoch 12/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.4916 - acc: 0.0199 - val_loss: 5.4519 - val_acc: 0.0000e+00\n",
            "Epoch 13/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.4857 - acc: 0.0000e+00 - val_loss: 5.4383 - val_acc: 0.0113\n",
            "Epoch 14/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.4682 - acc: 0.0132 - val_loss: 5.4241 - val_acc: 0.1972\n",
            "Epoch 15/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.4499 - acc: 0.0199 - val_loss: 5.4099 - val_acc: 0.5070\n",
            "Epoch 16/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.4409 - acc: 0.0397 - val_loss: 5.3951 - val_acc: 0.7718\n",
            "Epoch 17/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.4341 - acc: 0.0265 - val_loss: 5.3794 - val_acc: 0.8592\n",
            "Epoch 18/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.4248 - acc: 0.0265 - val_loss: 5.3638 - val_acc: 0.8986\n",
            "Epoch 19/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.3967 - acc: 0.0795 - val_loss: 5.3469 - val_acc: 0.9183\n",
            "Epoch 20/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.3945 - acc: 0.0464 - val_loss: 5.3291 - val_acc: 0.9268\n",
            "Epoch 21/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.3623 - acc: 0.0927 - val_loss: 5.3104 - val_acc: 0.9296\n",
            "Epoch 22/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.3350 - acc: 0.1854 - val_loss: 5.2899 - val_acc: 0.9296\n",
            "Epoch 23/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.3313 - acc: 0.1457 - val_loss: 5.2683 - val_acc: 0.9296\n",
            "Epoch 24/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.3288 - acc: 0.1722 - val_loss: 5.2457 - val_acc: 0.9296\n",
            "Epoch 25/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.3037 - acc: 0.2318 - val_loss: 5.2217 - val_acc: 0.9296\n",
            "Epoch 26/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.2900 - acc: 0.2583 - val_loss: 5.1961 - val_acc: 0.9296\n",
            "Epoch 27/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.2595 - acc: 0.3245 - val_loss: 5.1683 - val_acc: 0.9296\n",
            "Epoch 28/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.2351 - acc: 0.3179 - val_loss: 5.1368 - val_acc: 0.9296\n",
            "Epoch 29/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.2011 - acc: 0.3974 - val_loss: 5.1014 - val_acc: 0.9296\n",
            "Epoch 30/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.1559 - acc: 0.5298 - val_loss: 5.0620 - val_acc: 0.9296\n",
            "Epoch 31/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.1359 - acc: 0.5430 - val_loss: 5.0199 - val_acc: 0.9296\n",
            "Epoch 32/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.1021 - acc: 0.5232 - val_loss: 4.9716 - val_acc: 0.9296\n",
            "Epoch 33/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 5.0443 - acc: 0.6689 - val_loss: 4.9166 - val_acc: 0.9296\n",
            "Epoch 34/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 4.9877 - acc: 0.7285 - val_loss: 4.8527 - val_acc: 0.9296\n",
            "Epoch 35/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 4.9484 - acc: 0.6954 - val_loss: 4.7798 - val_acc: 0.9296\n",
            "Epoch 36/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 4.8630 - acc: 0.7947 - val_loss: 4.6941 - val_acc: 0.9296\n",
            "Epoch 37/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 4.7682 - acc: 0.8013 - val_loss: 4.5909 - val_acc: 0.9296\n",
            "Epoch 38/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 4.6953 - acc: 0.8212 - val_loss: 4.4681 - val_acc: 0.9296\n",
            "Epoch 39/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 4.5727 - acc: 0.8411 - val_loss: 4.3171 - val_acc: 0.9296\n",
            "Epoch 40/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 4.4679 - acc: 0.8543 - val_loss: 4.1382 - val_acc: 0.9296\n",
            "Epoch 41/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 4.2447 - acc: 0.9007 - val_loss: 3.9018 - val_acc: 0.9296\n",
            "Epoch 42/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 4.0507 - acc: 0.8940 - val_loss: 3.6004 - val_acc: 0.9296\n",
            "Epoch 43/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 3.7463 - acc: 0.9139 - val_loss: 3.2007 - val_acc: 0.9296\n",
            "Epoch 44/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 3.3309 - acc: 0.9139 - val_loss: 2.6888 - val_acc: 0.9296\n",
            "Epoch 45/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 2.9453 - acc: 0.9007 - val_loss: 2.0956 - val_acc: 0.9296\n",
            "Epoch 46/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 2.3642 - acc: 0.9139 - val_loss: 1.5255 - val_acc: 0.9296\n",
            "Epoch 47/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.8358 - acc: 0.9205 - val_loss: 1.1495 - val_acc: 0.9296\n",
            "Epoch 48/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.5117 - acc: 0.9205 - val_loss: 0.9446 - val_acc: 0.9296\n",
            "Epoch 49/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.3560 - acc: 0.9205 - val_loss: 0.8493 - val_acc: 0.9296\n",
            "Epoch 50/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.2060 - acc: 0.9205 - val_loss: 0.8079 - val_acc: 0.9296\n",
            "Epoch 51/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.1281 - acc: 0.9205 - val_loss: 0.7913 - val_acc: 0.9296\n",
            "Epoch 52/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.1078 - acc: 0.9205 - val_loss: 0.7856 - val_acc: 0.9296\n",
            "Epoch 53/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.1100 - acc: 0.9205 - val_loss: 0.7829 - val_acc: 0.9296\n",
            "Epoch 54/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.0874 - acc: 0.9205 - val_loss: 0.7815 - val_acc: 0.9296\n",
            "Epoch 55/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.1199 - acc: 0.9205 - val_loss: 0.7810 - val_acc: 0.9296\n",
            "Epoch 56/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.0667 - acc: 0.9205 - val_loss: 0.7814 - val_acc: 0.9296\n",
            "Epoch 57/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.0619 - acc: 0.9205 - val_loss: 0.7817 - val_acc: 0.9296\n",
            "Epoch 58/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.0399 - acc: 0.9205 - val_loss: 0.7811 - val_acc: 0.9296\n",
            "Epoch 59/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.0424 - acc: 0.9205 - val_loss: 0.7802 - val_acc: 0.9296\n",
            "Epoch 60/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.0732 - acc: 0.9205 - val_loss: 0.7806 - val_acc: 0.9296\n",
            "Epoch 61/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9335 - acc: 0.9205 - val_loss: 0.7804 - val_acc: 0.9296\n",
            "Epoch 62/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.1315 - acc: 0.9205 - val_loss: 0.7803 - val_acc: 0.9296\n",
            "Epoch 63/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9852 - acc: 0.9205 - val_loss: 0.7800 - val_acc: 0.9296\n",
            "Epoch 64/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.0364 - acc: 0.9205 - val_loss: 0.7787 - val_acc: 0.9296\n",
            "Epoch 65/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9922 - acc: 0.9205 - val_loss: 0.7790 - val_acc: 0.9296\n",
            "Epoch 66/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.0735 - acc: 0.9205 - val_loss: 0.7780 - val_acc: 0.9296\n",
            "Epoch 67/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9820 - acc: 0.9205 - val_loss: 0.7800 - val_acc: 0.9296\n",
            "Epoch 68/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.0532 - acc: 0.9205 - val_loss: 0.7797 - val_acc: 0.9296\n",
            "Epoch 69/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9927 - acc: 0.9205 - val_loss: 0.7795 - val_acc: 0.9296\n",
            "Epoch 70/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9229 - acc: 0.9205 - val_loss: 0.7791 - val_acc: 0.9296\n",
            "Epoch 71/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.0504 - acc: 0.9205 - val_loss: 0.7795 - val_acc: 0.9296\n",
            "Epoch 72/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.0207 - acc: 0.9205 - val_loss: 0.7809 - val_acc: 0.9296\n",
            "Epoch 73/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9703 - acc: 0.9205 - val_loss: 0.7800 - val_acc: 0.9296\n",
            "Epoch 74/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.1080 - acc: 0.9205 - val_loss: 0.7771 - val_acc: 0.9296\n",
            "Epoch 75/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.0046 - acc: 0.9205 - val_loss: 0.7766 - val_acc: 0.9296\n",
            "Epoch 76/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9812 - acc: 0.9205 - val_loss: 0.7750 - val_acc: 0.9296\n",
            "Epoch 77/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9734 - acc: 0.9205 - val_loss: 0.7750 - val_acc: 0.9296\n",
            "Epoch 78/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9883 - acc: 0.9205 - val_loss: 0.7746 - val_acc: 0.9296\n",
            "Epoch 79/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9495 - acc: 0.9205 - val_loss: 0.7744 - val_acc: 0.9296\n",
            "Epoch 80/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9558 - acc: 0.9205 - val_loss: 0.7728 - val_acc: 0.9296\n",
            "Epoch 81/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.0015 - acc: 0.9205 - val_loss: 0.7748 - val_acc: 0.9296\n",
            "Epoch 82/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9517 - acc: 0.9205 - val_loss: 0.7744 - val_acc: 0.9296\n",
            "Epoch 83/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9893 - acc: 0.9205 - val_loss: 0.7734 - val_acc: 0.9296\n",
            "Epoch 84/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9703 - acc: 0.9205 - val_loss: 0.7748 - val_acc: 0.9296\n",
            "Epoch 85/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.0104 - acc: 0.9205 - val_loss: 0.7755 - val_acc: 0.9296\n",
            "Epoch 86/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9834 - acc: 0.9205 - val_loss: 0.7730 - val_acc: 0.9296\n",
            "Epoch 87/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9867 - acc: 0.9205 - val_loss: 0.7714 - val_acc: 0.9296\n",
            "Epoch 88/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9575 - acc: 0.9205 - val_loss: 0.7711 - val_acc: 0.9296\n",
            "Epoch 89/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9445 - acc: 0.9205 - val_loss: 0.7701 - val_acc: 0.9296\n",
            "Epoch 90/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9428 - acc: 0.9205 - val_loss: 0.7702 - val_acc: 0.9296\n",
            "Epoch 91/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 1.0062 - acc: 0.9205 - val_loss: 0.7701 - val_acc: 0.9296\n",
            "Epoch 92/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9816 - acc: 0.9205 - val_loss: 0.7706 - val_acc: 0.9296\n",
            "Epoch 93/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9481 - acc: 0.9205 - val_loss: 0.7715 - val_acc: 0.9296\n",
            "Epoch 94/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9561 - acc: 0.9205 - val_loss: 0.7706 - val_acc: 0.9296\n",
            "Epoch 95/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9849 - acc: 0.9205 - val_loss: 0.7706 - val_acc: 0.9296\n",
            "Epoch 96/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9882 - acc: 0.9205 - val_loss: 0.7693 - val_acc: 0.9296\n",
            "Epoch 97/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9838 - acc: 0.9205 - val_loss: 0.7689 - val_acc: 0.9296\n",
            "Epoch 98/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9882 - acc: 0.9205 - val_loss: 0.7695 - val_acc: 0.9296\n",
            "Epoch 99/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9724 - acc: 0.9205 - val_loss: 0.7694 - val_acc: 0.9296\n",
            "Epoch 100/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9502 - acc: 0.9205 - val_loss: 0.7687 - val_acc: 0.9296\n",
            "Epoch 101/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9275 - acc: 0.9205 - val_loss: 0.7717 - val_acc: 0.9296\n",
            "Epoch 102/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9122 - acc: 0.9205 - val_loss: 0.7720 - val_acc: 0.9296\n",
            "Epoch 103/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8904 - acc: 0.9205 - val_loss: 0.7734 - val_acc: 0.9296\n",
            "Epoch 104/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9243 - acc: 0.9205 - val_loss: 0.7738 - val_acc: 0.9296\n",
            "Epoch 105/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9684 - acc: 0.9205 - val_loss: 0.7721 - val_acc: 0.9296\n",
            "Epoch 106/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9311 - acc: 0.9205 - val_loss: 0.7724 - val_acc: 0.9296\n",
            "Epoch 107/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9605 - acc: 0.9205 - val_loss: 0.7696 - val_acc: 0.9296\n",
            "Epoch 108/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8506 - acc: 0.9205 - val_loss: 0.7722 - val_acc: 0.9296\n",
            "Epoch 109/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9003 - acc: 0.9205 - val_loss: 0.7725 - val_acc: 0.9296\n",
            "Epoch 110/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8723 - acc: 0.9205 - val_loss: 0.7716 - val_acc: 0.9296\n",
            "Epoch 111/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9777 - acc: 0.9205 - val_loss: 0.7730 - val_acc: 0.9296\n",
            "Epoch 112/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9267 - acc: 0.9205 - val_loss: 0.7740 - val_acc: 0.9296\n",
            "Epoch 113/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8895 - acc: 0.9205 - val_loss: 0.7719 - val_acc: 0.9296\n",
            "Epoch 114/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8654 - acc: 0.9205 - val_loss: 0.7720 - val_acc: 0.9296\n",
            "Epoch 115/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9009 - acc: 0.9205 - val_loss: 0.7727 - val_acc: 0.9296\n",
            "Epoch 116/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9170 - acc: 0.9205 - val_loss: 0.7727 - val_acc: 0.9296\n",
            "Epoch 117/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8875 - acc: 0.9205 - val_loss: 0.7723 - val_acc: 0.9296\n",
            "Epoch 118/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9144 - acc: 0.9205 - val_loss: 0.7712 - val_acc: 0.9296\n",
            "Epoch 119/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8606 - acc: 0.9205 - val_loss: 0.7721 - val_acc: 0.9296\n",
            "Epoch 120/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8828 - acc: 0.9205 - val_loss: 0.7725 - val_acc: 0.9296\n",
            "Epoch 121/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9247 - acc: 0.9205 - val_loss: 0.7753 - val_acc: 0.9296\n",
            "Epoch 122/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8769 - acc: 0.9205 - val_loss: 0.7729 - val_acc: 0.9296\n",
            "Epoch 123/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9300 - acc: 0.9205 - val_loss: 0.7737 - val_acc: 0.9296\n",
            "Epoch 124/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9124 - acc: 0.9205 - val_loss: 0.7711 - val_acc: 0.9296\n",
            "Epoch 125/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9142 - acc: 0.9205 - val_loss: 0.7709 - val_acc: 0.9296\n",
            "Epoch 126/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9103 - acc: 0.9205 - val_loss: 0.7719 - val_acc: 0.9296\n",
            "Epoch 127/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9346 - acc: 0.9205 - val_loss: 0.7714 - val_acc: 0.9296\n",
            "Epoch 128/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8732 - acc: 0.9205 - val_loss: 0.7725 - val_acc: 0.9296\n",
            "Epoch 129/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9076 - acc: 0.9205 - val_loss: 0.7721 - val_acc: 0.9296\n",
            "Epoch 130/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8737 - acc: 0.9205 - val_loss: 0.7738 - val_acc: 0.9296\n",
            "Epoch 131/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8693 - acc: 0.9205 - val_loss: 0.7743 - val_acc: 0.9296\n",
            "Epoch 132/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9457 - acc: 0.9205 - val_loss: 0.7746 - val_acc: 0.9296\n",
            "Epoch 133/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8742 - acc: 0.9205 - val_loss: 0.7747 - val_acc: 0.9296\n",
            "Epoch 134/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9475 - acc: 0.9205 - val_loss: 0.7721 - val_acc: 0.9296\n",
            "Epoch 135/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8697 - acc: 0.9205 - val_loss: 0.7712 - val_acc: 0.9296\n",
            "Epoch 136/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8957 - acc: 0.9205 - val_loss: 0.7722 - val_acc: 0.9296\n",
            "Epoch 137/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9381 - acc: 0.9205 - val_loss: 0.7702 - val_acc: 0.9296\n",
            "Epoch 138/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8945 - acc: 0.9205 - val_loss: 0.7712 - val_acc: 0.9296\n",
            "Epoch 139/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9136 - acc: 0.9205 - val_loss: 0.7724 - val_acc: 0.9296\n",
            "Epoch 140/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9271 - acc: 0.9205 - val_loss: 0.7711 - val_acc: 0.9296\n",
            "Epoch 141/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9085 - acc: 0.9205 - val_loss: 0.7705 - val_acc: 0.9296\n",
            "Epoch 142/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8979 - acc: 0.9205 - val_loss: 0.7698 - val_acc: 0.9296\n",
            "Epoch 143/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8565 - acc: 0.9205 - val_loss: 0.7704 - val_acc: 0.9296\n",
            "Epoch 144/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9088 - acc: 0.9205 - val_loss: 0.7688 - val_acc: 0.9296\n",
            "Epoch 145/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8686 - acc: 0.9205 - val_loss: 0.7688 - val_acc: 0.9296\n",
            "Epoch 146/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8648 - acc: 0.9205 - val_loss: 0.7697 - val_acc: 0.9296\n",
            "Epoch 147/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8772 - acc: 0.9205 - val_loss: 0.7699 - val_acc: 0.9296\n",
            "Epoch 148/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8529 - acc: 0.9205 - val_loss: 0.7708 - val_acc: 0.9296\n",
            "Epoch 149/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8697 - acc: 0.9205 - val_loss: 0.7718 - val_acc: 0.9296\n",
            "Epoch 150/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9117 - acc: 0.9205 - val_loss: 0.7695 - val_acc: 0.9296\n",
            "Epoch 151/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8624 - acc: 0.9205 - val_loss: 0.7699 - val_acc: 0.9296\n",
            "Epoch 152/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8603 - acc: 0.9205 - val_loss: 0.7719 - val_acc: 0.9296\n",
            "Epoch 153/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8897 - acc: 0.9205 - val_loss: 0.7728 - val_acc: 0.9296\n",
            "Epoch 154/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8838 - acc: 0.9205 - val_loss: 0.7735 - val_acc: 0.9296\n",
            "Epoch 155/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8127 - acc: 0.9205 - val_loss: 0.7723 - val_acc: 0.9296\n",
            "Epoch 156/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8406 - acc: 0.9205 - val_loss: 0.7713 - val_acc: 0.9296\n",
            "Epoch 157/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8704 - acc: 0.9205 - val_loss: 0.7715 - val_acc: 0.9296\n",
            "Epoch 158/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8975 - acc: 0.9205 - val_loss: 0.7711 - val_acc: 0.9296\n",
            "Epoch 159/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8835 - acc: 0.9205 - val_loss: 0.7711 - val_acc: 0.9296\n",
            "Epoch 160/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9185 - acc: 0.9205 - val_loss: 0.7702 - val_acc: 0.9296\n",
            "Epoch 161/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8057 - acc: 0.9205 - val_loss: 0.7717 - val_acc: 0.9296\n",
            "Epoch 162/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8520 - acc: 0.9205 - val_loss: 0.7722 - val_acc: 0.9296\n",
            "Epoch 163/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9099 - acc: 0.9205 - val_loss: 0.7717 - val_acc: 0.9296\n",
            "Epoch 164/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9212 - acc: 0.9205 - val_loss: 0.7716 - val_acc: 0.9296\n",
            "Epoch 165/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8877 - acc: 0.9205 - val_loss: 0.7701 - val_acc: 0.9296\n",
            "Epoch 166/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9225 - acc: 0.9205 - val_loss: 0.7698 - val_acc: 0.9296\n",
            "Epoch 167/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8446 - acc: 0.9205 - val_loss: 0.7702 - val_acc: 0.9296\n",
            "Epoch 168/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8288 - acc: 0.9205 - val_loss: 0.7710 - val_acc: 0.9296\n",
            "Epoch 169/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9021 - acc: 0.9205 - val_loss: 0.7713 - val_acc: 0.9296\n",
            "Epoch 170/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8891 - acc: 0.9205 - val_loss: 0.7708 - val_acc: 0.9296\n",
            "Epoch 171/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8712 - acc: 0.9205 - val_loss: 0.7714 - val_acc: 0.9296\n",
            "Epoch 172/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8610 - acc: 0.9205 - val_loss: 0.7694 - val_acc: 0.9296\n",
            "Epoch 173/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8269 - acc: 0.9205 - val_loss: 0.7683 - val_acc: 0.9296\n",
            "Epoch 174/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8717 - acc: 0.9205 - val_loss: 0.7684 - val_acc: 0.9296\n",
            "Epoch 175/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8682 - acc: 0.9205 - val_loss: 0.7669 - val_acc: 0.9296\n",
            "Epoch 176/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8104 - acc: 0.9205 - val_loss: 0.7668 - val_acc: 0.9296\n",
            "Epoch 177/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8714 - acc: 0.9205 - val_loss: 0.7688 - val_acc: 0.9296\n",
            "Epoch 178/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8315 - acc: 0.9205 - val_loss: 0.7702 - val_acc: 0.9296\n",
            "Epoch 179/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8721 - acc: 0.9205 - val_loss: 0.7695 - val_acc: 0.9296\n",
            "Epoch 180/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8687 - acc: 0.9205 - val_loss: 0.7690 - val_acc: 0.9296\n",
            "Epoch 181/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8928 - acc: 0.9205 - val_loss: 0.7694 - val_acc: 0.9296\n",
            "Epoch 182/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8867 - acc: 0.9205 - val_loss: 0.7684 - val_acc: 0.9296\n",
            "Epoch 183/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8770 - acc: 0.9205 - val_loss: 0.7679 - val_acc: 0.9296\n",
            "Epoch 184/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.9226 - acc: 0.9205 - val_loss: 0.7681 - val_acc: 0.9296\n",
            "Epoch 185/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8916 - acc: 0.9205 - val_loss: 0.7668 - val_acc: 0.9296\n",
            "Epoch 186/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8468 - acc: 0.9205 - val_loss: 0.7657 - val_acc: 0.9296\n",
            "Epoch 187/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8774 - acc: 0.9205 - val_loss: 0.7660 - val_acc: 0.9296\n",
            "Epoch 188/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8239 - acc: 0.9205 - val_loss: 0.7678 - val_acc: 0.9296\n",
            "Epoch 189/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8835 - acc: 0.9205 - val_loss: 0.7655 - val_acc: 0.9296\n",
            "Epoch 190/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8352 - acc: 0.9205 - val_loss: 0.7653 - val_acc: 0.9296\n",
            "Epoch 191/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8828 - acc: 0.9205 - val_loss: 0.7669 - val_acc: 0.9296\n",
            "Epoch 192/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8253 - acc: 0.9205 - val_loss: 0.7677 - val_acc: 0.9296\n",
            "Epoch 193/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8410 - acc: 0.9205 - val_loss: 0.7667 - val_acc: 0.9296\n",
            "Epoch 194/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8714 - acc: 0.9205 - val_loss: 0.7649 - val_acc: 0.9296\n",
            "Epoch 195/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8497 - acc: 0.9205 - val_loss: 0.7651 - val_acc: 0.9296\n",
            "Epoch 196/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8812 - acc: 0.9205 - val_loss: 0.7643 - val_acc: 0.9296\n",
            "Epoch 197/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8824 - acc: 0.9205 - val_loss: 0.7658 - val_acc: 0.9296\n",
            "Epoch 198/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8395 - acc: 0.9205 - val_loss: 0.7687 - val_acc: 0.9296\n",
            "Epoch 199/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8375 - acc: 0.9205 - val_loss: 0.7710 - val_acc: 0.9296\n",
            "Epoch 200/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8115 - acc: 0.9205 - val_loss: 0.7693 - val_acc: 0.9296\n",
            "Epoch 201/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8458 - acc: 0.9205 - val_loss: 0.7679 - val_acc: 0.9296\n",
            "Epoch 202/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8698 - acc: 0.9205 - val_loss: 0.7676 - val_acc: 0.9296\n",
            "Epoch 203/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8086 - acc: 0.9205 - val_loss: 0.7684 - val_acc: 0.9296\n",
            "Epoch 204/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8451 - acc: 0.9205 - val_loss: 0.7668 - val_acc: 0.9296\n",
            "Epoch 205/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8586 - acc: 0.9205 - val_loss: 0.7674 - val_acc: 0.9296\n",
            "Epoch 206/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7996 - acc: 0.9205 - val_loss: 0.7663 - val_acc: 0.9296\n",
            "Epoch 207/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8404 - acc: 0.9205 - val_loss: 0.7681 - val_acc: 0.9296\n",
            "Epoch 208/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8887 - acc: 0.9205 - val_loss: 0.7678 - val_acc: 0.9296\n",
            "Epoch 209/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7716 - acc: 0.9205 - val_loss: 0.7686 - val_acc: 0.9296\n",
            "Epoch 210/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7703 - acc: 0.9205 - val_loss: 0.7665 - val_acc: 0.9296\n",
            "Epoch 211/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8084 - acc: 0.9205 - val_loss: 0.7679 - val_acc: 0.9296\n",
            "Epoch 212/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8317 - acc: 0.9205 - val_loss: 0.7682 - val_acc: 0.9296\n",
            "Epoch 213/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8574 - acc: 0.9205 - val_loss: 0.7683 - val_acc: 0.9296\n",
            "Epoch 214/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7868 - acc: 0.9205 - val_loss: 0.7702 - val_acc: 0.9296\n",
            "Epoch 215/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8253 - acc: 0.9205 - val_loss: 0.7700 - val_acc: 0.9296\n",
            "Epoch 216/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8906 - acc: 0.9205 - val_loss: 0.7682 - val_acc: 0.9296\n",
            "Epoch 217/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8137 - acc: 0.9205 - val_loss: 0.7682 - val_acc: 0.9296\n",
            "Epoch 218/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8441 - acc: 0.9205 - val_loss: 0.7686 - val_acc: 0.9296\n",
            "Epoch 219/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8431 - acc: 0.9205 - val_loss: 0.7663 - val_acc: 0.9296\n",
            "Epoch 220/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8412 - acc: 0.9205 - val_loss: 0.7664 - val_acc: 0.9296\n",
            "Epoch 221/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8002 - acc: 0.9205 - val_loss: 0.7691 - val_acc: 0.9296\n",
            "Epoch 222/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8024 - acc: 0.9205 - val_loss: 0.7703 - val_acc: 0.9296\n",
            "Epoch 223/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8332 - acc: 0.9205 - val_loss: 0.7684 - val_acc: 0.9296\n",
            "Epoch 224/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8183 - acc: 0.9205 - val_loss: 0.7676 - val_acc: 0.9296\n",
            "Epoch 225/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8193 - acc: 0.9205 - val_loss: 0.7653 - val_acc: 0.9296\n",
            "Epoch 226/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8741 - acc: 0.9205 - val_loss: 0.7661 - val_acc: 0.9296\n",
            "Epoch 227/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8687 - acc: 0.9205 - val_loss: 0.7665 - val_acc: 0.9296\n",
            "Epoch 228/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8142 - acc: 0.9205 - val_loss: 0.7661 - val_acc: 0.9296\n",
            "Epoch 229/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8007 - acc: 0.9205 - val_loss: 0.7667 - val_acc: 0.9296\n",
            "Epoch 230/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7990 - acc: 0.9205 - val_loss: 0.7649 - val_acc: 0.9296\n",
            "Epoch 231/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7680 - acc: 0.9205 - val_loss: 0.7652 - val_acc: 0.9296\n",
            "Epoch 232/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7931 - acc: 0.9205 - val_loss: 0.7655 - val_acc: 0.9296\n",
            "Epoch 233/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8474 - acc: 0.9205 - val_loss: 0.7654 - val_acc: 0.9296\n",
            "Epoch 234/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7875 - acc: 0.9205 - val_loss: 0.7657 - val_acc: 0.9296\n",
            "Epoch 235/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7579 - acc: 0.9205 - val_loss: 0.7671 - val_acc: 0.9296\n",
            "Epoch 236/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7207 - acc: 0.9205 - val_loss: 0.7714 - val_acc: 0.9296\n",
            "Epoch 237/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8207 - acc: 0.9205 - val_loss: 0.7689 - val_acc: 0.9296\n",
            "Epoch 238/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8546 - acc: 0.9205 - val_loss: 0.7674 - val_acc: 0.9296\n",
            "Epoch 239/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8133 - acc: 0.9205 - val_loss: 0.7674 - val_acc: 0.9296\n",
            "Epoch 240/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8167 - acc: 0.9205 - val_loss: 0.7646 - val_acc: 0.9296\n",
            "Epoch 241/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8426 - acc: 0.9205 - val_loss: 0.7647 - val_acc: 0.9296\n",
            "Epoch 242/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8494 - acc: 0.9205 - val_loss: 0.7641 - val_acc: 0.9296\n",
            "Epoch 243/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8337 - acc: 0.9205 - val_loss: 0.7636 - val_acc: 0.9296\n",
            "Epoch 244/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8062 - acc: 0.9205 - val_loss: 0.7622 - val_acc: 0.9296\n",
            "Epoch 245/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8223 - acc: 0.9205 - val_loss: 0.7641 - val_acc: 0.9296\n",
            "Epoch 246/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8313 - acc: 0.9205 - val_loss: 0.7615 - val_acc: 0.9296\n",
            "Epoch 247/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8523 - acc: 0.9205 - val_loss: 0.7621 - val_acc: 0.9296\n",
            "Epoch 248/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7649 - acc: 0.9205 - val_loss: 0.7646 - val_acc: 0.9296\n",
            "Epoch 249/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8237 - acc: 0.9205 - val_loss: 0.7635 - val_acc: 0.9296\n",
            "Epoch 250/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7691 - acc: 0.9205 - val_loss: 0.7650 - val_acc: 0.9296\n",
            "Epoch 251/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7932 - acc: 0.9205 - val_loss: 0.7660 - val_acc: 0.9296\n",
            "Epoch 252/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8042 - acc: 0.9205 - val_loss: 0.7653 - val_acc: 0.9296\n",
            "Epoch 253/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7735 - acc: 0.9205 - val_loss: 0.7664 - val_acc: 0.9296\n",
            "Epoch 254/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8339 - acc: 0.9205 - val_loss: 0.7654 - val_acc: 0.9296\n",
            "Epoch 255/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7449 - acc: 0.9205 - val_loss: 0.7659 - val_acc: 0.9296\n",
            "Epoch 256/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7706 - acc: 0.9205 - val_loss: 0.7641 - val_acc: 0.9296\n",
            "Epoch 257/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7734 - acc: 0.9205 - val_loss: 0.7634 - val_acc: 0.9296\n",
            "Epoch 258/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8085 - acc: 0.9205 - val_loss: 0.7606 - val_acc: 0.9296\n",
            "Epoch 259/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8185 - acc: 0.9205 - val_loss: 0.7625 - val_acc: 0.9296\n",
            "Epoch 260/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8219 - acc: 0.9205 - val_loss: 0.7624 - val_acc: 0.9296\n",
            "Epoch 261/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7406 - acc: 0.9205 - val_loss: 0.7631 - val_acc: 0.9296\n",
            "Epoch 262/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7740 - acc: 0.9205 - val_loss: 0.7627 - val_acc: 0.9296\n",
            "Epoch 263/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8252 - acc: 0.9205 - val_loss: 0.7614 - val_acc: 0.9296\n",
            "Epoch 264/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7784 - acc: 0.9205 - val_loss: 0.7647 - val_acc: 0.9296\n",
            "Epoch 265/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7932 - acc: 0.9205 - val_loss: 0.7653 - val_acc: 0.9296\n",
            "Epoch 266/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7469 - acc: 0.9205 - val_loss: 0.7682 - val_acc: 0.9296\n",
            "Epoch 267/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7933 - acc: 0.9205 - val_loss: 0.7685 - val_acc: 0.9296\n",
            "Epoch 268/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8035 - acc: 0.9205 - val_loss: 0.7672 - val_acc: 0.9296\n",
            "Epoch 269/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8115 - acc: 0.9205 - val_loss: 0.7678 - val_acc: 0.9296\n",
            "Epoch 270/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8226 - acc: 0.9205 - val_loss: 0.7658 - val_acc: 0.9296\n",
            "Epoch 271/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8160 - acc: 0.9205 - val_loss: 0.7662 - val_acc: 0.9296\n",
            "Epoch 272/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7769 - acc: 0.9205 - val_loss: 0.7665 - val_acc: 0.9296\n",
            "Epoch 273/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8120 - acc: 0.9205 - val_loss: 0.7671 - val_acc: 0.9296\n",
            "Epoch 274/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8045 - acc: 0.9205 - val_loss: 0.7668 - val_acc: 0.9296\n",
            "Epoch 275/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7909 - acc: 0.9205 - val_loss: 0.7663 - val_acc: 0.9296\n",
            "Epoch 276/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7170 - acc: 0.9205 - val_loss: 0.7663 - val_acc: 0.9296\n",
            "Epoch 277/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8101 - acc: 0.9205 - val_loss: 0.7652 - val_acc: 0.9296\n",
            "Epoch 278/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8304 - acc: 0.9205 - val_loss: 0.7677 - val_acc: 0.9296\n",
            "Epoch 279/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8028 - acc: 0.9205 - val_loss: 0.7689 - val_acc: 0.9296\n",
            "Epoch 280/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7691 - acc: 0.9205 - val_loss: 0.7704 - val_acc: 0.9296\n",
            "Epoch 281/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7916 - acc: 0.9205 - val_loss: 0.7697 - val_acc: 0.9296\n",
            "Epoch 282/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7574 - acc: 0.9205 - val_loss: 0.7683 - val_acc: 0.9296\n",
            "Epoch 283/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7889 - acc: 0.9205 - val_loss: 0.7690 - val_acc: 0.9296\n",
            "Epoch 284/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7940 - acc: 0.9205 - val_loss: 0.7659 - val_acc: 0.9296\n",
            "Epoch 285/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7172 - acc: 0.9205 - val_loss: 0.7647 - val_acc: 0.9296\n",
            "Epoch 286/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7715 - acc: 0.9205 - val_loss: 0.7656 - val_acc: 0.9296\n",
            "Epoch 287/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8265 - acc: 0.9205 - val_loss: 0.7639 - val_acc: 0.9296\n",
            "Epoch 288/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7826 - acc: 0.9205 - val_loss: 0.7639 - val_acc: 0.9296\n",
            "Epoch 289/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8138 - acc: 0.9205 - val_loss: 0.7639 - val_acc: 0.9296\n",
            "Epoch 290/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7685 - acc: 0.9205 - val_loss: 0.7630 - val_acc: 0.9296\n",
            "Epoch 291/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8036 - acc: 0.9205 - val_loss: 0.7622 - val_acc: 0.9296\n",
            "Epoch 292/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8270 - acc: 0.9205 - val_loss: 0.7643 - val_acc: 0.9296\n",
            "Epoch 293/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7518 - acc: 0.9205 - val_loss: 0.7656 - val_acc: 0.9296\n",
            "Epoch 294/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7820 - acc: 0.9205 - val_loss: 0.7643 - val_acc: 0.9296\n",
            "Epoch 295/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7062 - acc: 0.9205 - val_loss: 0.7639 - val_acc: 0.9296\n",
            "Epoch 296/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7638 - acc: 0.9205 - val_loss: 0.7647 - val_acc: 0.9296\n",
            "Epoch 297/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7256 - acc: 0.9205 - val_loss: 0.7682 - val_acc: 0.9296\n",
            "Epoch 298/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7888 - acc: 0.9205 - val_loss: 0.7660 - val_acc: 0.9296\n",
            "Epoch 299/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.8533 - acc: 0.9205 - val_loss: 0.7634 - val_acc: 0.9296\n",
            "Epoch 300/300\n",
            "151/151 [==============================] - 3s 21ms/step - loss: 0.7170 - acc: 0.9205 - val_loss: 0.7649 - val_acc: 0.9296\n",
            "Accuracy: 92.96%\n",
            "Saved model to disk\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEICAYAAAB25L6yAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIaKcBLUQQkQ5CWohhIhyEtRCCBHl/h+m2WTDWjbpXAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}