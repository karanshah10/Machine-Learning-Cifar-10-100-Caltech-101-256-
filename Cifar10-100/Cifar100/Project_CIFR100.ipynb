{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_CIFR100.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7mcJgP5brDY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "13cacbfe-685e-46ac-94f8-9b25a0597e3a"
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import cifar100\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.constraints import maxnorm\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "K.common.set_image_dim_ordering('th')\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.callbacks import EarlyStopping\n",
        "from getdatasets import getCifar100Data\n",
        "from cifar100_model import cifar100_model\n",
        "from utilities import cifar100_augment\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_cifar100(x_train, y_train, x_test, y_test,augmentData, model):\n",
        "  batch_size = 128\n",
        "  maxepoches = 250\n",
        "  learning_rate = 0.1\n",
        "  lr_decay = 1e-6\n",
        "  lrf = learning_rate\n",
        "  sgd = SGD(lr=lrf, decay=lr_decay, momentum=0.9, nesterov=True)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
        "  for epoch in range(1,150):\n",
        "\n",
        "    if epoch%25==0 and epoch>0:\n",
        "        lrf/=2\n",
        "        sgd = SGD(lr=lrf, decay=lr_decay, momentum=0.9, nesterov=True)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    trainingResult = model.fit_generator(augmentData.flow(x_train, y_train,batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,epochs=epoch, verbose=1,validation_data=(x_test, y_test),initial_epoch=epoch-1)\n",
        "  return trainingResult\n",
        "\n",
        "def evaluate_cifar100(x_test, y_test, model):\n",
        "  scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "  print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "  return scores \n",
        "\n",
        "if __name__ == '__main__':\n",
        "  x_train, y_train, x_test, y_test = getCifar100Data()\n",
        "  num_classes = y_test.shape[1]\n",
        "  model = cifar100_model(num_classes)\n",
        "  model.summary()\n",
        "  augmentData = cifar100_augment(x_train)\n",
        "  trainingResult= train_cifar100(x_train, y_train, x_test, y_test,augmentData, model)\n",
        "  testResult = evaluate_cifar100(x_test, y_test, model)\n",
        "  plt.plot(trainingResult.history['loss'])\n",
        "  plt.plot(trainingResult.history['val_loss'])\n",
        "  plt.legend(['train','test'])\n",
        "  plt.title('loss')\n",
        "  plt.figure()\n",
        "  plt.plot(trainingResult.history['acc'])\n",
        "  plt.plot(trainingResult.history['val_acc'])\n",
        "  plt.legend(['train','test'])\n",
        "  plt.title('accuracy')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "121.93584\n",
            "68.38902\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_21 (Conv2D)           (None, 64, 32, 32)        1792      \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 64, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 64, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 64, 32, 32)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 64, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 64, 16, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 128, 16, 16)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 128, 16, 16)       64        \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 128, 16, 16)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 128, 16, 16)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 128, 16, 16)       64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 128, 8, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 256, 8, 8)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 256, 8, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 256, 8, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 256, 8, 8)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 256, 8, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 256, 8, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 256, 8, 8)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 256, 8, 8)         32        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 256, 4, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 512, 4, 4)         1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 512, 4, 4)         16        \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 512, 4, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 512, 4, 4)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 512, 4, 4)         16        \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 512, 4, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 512, 4, 4)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 512, 4, 4)         16        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 512, 2, 2)         0         \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 512, 2, 2)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               1049088   \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 100)               51300     \n",
            "=================================================================\n",
            "Total params: 8,738,228\n",
            "Trainable params: 8,736,940\n",
            "Non-trainable params: 1,288\n",
            "_________________________________________________________________\n",
            "Epoch 1/1\n",
            "390/390 [==============================] - 70s 178ms/step - loss: 7.1836 - acc: 0.0104 - val_loss: 6.3424 - val_acc: 0.0100\n",
            "Epoch 2/2\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 5.7822 - acc: 0.0099 - val_loss: 5.4775 - val_acc: 0.0100\n",
            "Epoch 3/3\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 5.1646 - acc: 0.0104 - val_loss: 4.9812 - val_acc: 0.0100\n",
            "Epoch 4/4\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 4.8872 - acc: 0.0102 - val_loss: 4.8027 - val_acc: 0.0100\n",
            "Epoch 5/5\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 4.7679 - acc: 0.0095 - val_loss: 4.7247 - val_acc: 0.0100\n",
            "Epoch 6/6\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 4.7123 - acc: 0.0104 - val_loss: 5.0559 - val_acc: 0.0100\n",
            "Epoch 7/7\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 4.6829 - acc: 0.0097 - val_loss: 4.6639 - val_acc: 0.0100\n",
            "Epoch 8/8\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 4.6642 - acc: 0.0095 - val_loss: 4.8344 - val_acc: 0.0100\n",
            "Epoch 9/9\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 4.6611 - acc: 0.0100 - val_loss: 4.6469 - val_acc: 0.0100\n",
            "Epoch 10/10\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 4.6581 - acc: 0.0095 - val_loss: 4.9291 - val_acc: 0.0100\n",
            "Epoch 11/11\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 5.3967 - acc: 0.0193 - val_loss: 11.0596 - val_acc: 0.0138\n",
            "Epoch 12/12\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 5.7455 - acc: 0.0315 - val_loss: 7.6256 - val_acc: 0.0206\n",
            "Epoch 13/13\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 5.4280 - acc: 0.0427 - val_loss: 5.0810 - val_acc: 0.0427\n",
            "Epoch 14/14\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 4.8950 - acc: 0.0583 - val_loss: 4.9167 - val_acc: 0.0651\n",
            "Epoch 15/15\n",
            "390/390 [==============================] - 63s 163ms/step - loss: 4.5217 - acc: 0.0820 - val_loss: 4.2471 - val_acc: 0.1140\n",
            "Epoch 16/16\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 4.1859 - acc: 0.1148 - val_loss: 3.8778 - val_acc: 0.1611\n",
            "Epoch 17/17\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.9868 - acc: 0.1387 - val_loss: 3.8055 - val_acc: 0.1861\n",
            "Epoch 18/18\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.9531 - acc: 0.1600 - val_loss: 3.9060 - val_acc: 0.1810\n",
            "Epoch 19/19\n",
            "390/390 [==============================] - 63s 163ms/step - loss: 3.9096 - acc: 0.1819 - val_loss: 3.8916 - val_acc: 0.1858\n",
            "Epoch 20/20\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 3.8274 - acc: 0.2034 - val_loss: 3.5811 - val_acc: 0.2578\n",
            "Epoch 21/21\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 3.7568 - acc: 0.2244 - val_loss: 3.4823 - val_acc: 0.2860\n",
            "Epoch 22/22\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 3.7502 - acc: 0.2454 - val_loss: 4.6569 - val_acc: 0.2192\n",
            "Epoch 23/23\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 3.8111 - acc: 0.2657 - val_loss: 3.6397 - val_acc: 0.2969\n",
            "Epoch 24/24\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.6418 - acc: 0.2912 - val_loss: 3.3987 - val_acc: 0.3510\n",
            "Epoch 25/25\n",
            "390/390 [==============================] - 70s 179ms/step - loss: 3.3127 - acc: 0.3485 - val_loss: 3.0795 - val_acc: 0.3914\n",
            "Epoch 26/26\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.1640 - acc: 0.3665 - val_loss: 3.0404 - val_acc: 0.3988\n",
            "Epoch 27/27\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.1029 - acc: 0.3785 - val_loss: 3.1048 - val_acc: 0.3825\n",
            "Epoch 28/28\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.0748 - acc: 0.3887 - val_loss: 2.8500 - val_acc: 0.4361\n",
            "Epoch 29/29\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.0504 - acc: 0.3973 - val_loss: 2.8180 - val_acc: 0.4521\n",
            "Epoch 30/30\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.0362 - acc: 0.4105 - val_loss: 3.0546 - val_acc: 0.4252\n",
            "Epoch 31/31\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.0347 - acc: 0.4160 - val_loss: 2.8692 - val_acc: 0.4593\n",
            "Epoch 32/32\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.0267 - acc: 0.4240 - val_loss: 2.8273 - val_acc: 0.4733\n",
            "Epoch 33/33\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.0161 - acc: 0.4320 - val_loss: 3.0187 - val_acc: 0.4494\n",
            "Epoch 34/34\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.0195 - acc: 0.4385 - val_loss: 2.9318 - val_acc: 0.4652\n",
            "Epoch 35/35\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.0085 - acc: 0.4448 - val_loss: 2.9191 - val_acc: 0.4684\n",
            "Epoch 36/36\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.0090 - acc: 0.4522 - val_loss: 2.9872 - val_acc: 0.4666\n",
            "Epoch 37/37\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.0145 - acc: 0.4567 - val_loss: 2.9263 - val_acc: 0.4808\n",
            "Epoch 38/38\n",
            "390/390 [==============================] - 63s 163ms/step - loss: 3.0079 - acc: 0.4616 - val_loss: 2.9204 - val_acc: 0.4928\n",
            "Epoch 39/39\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.0174 - acc: 0.4637 - val_loss: 2.9109 - val_acc: 0.4938\n",
            "Epoch 40/40\n",
            "390/390 [==============================] - 64s 164ms/step - loss: 3.0115 - acc: 0.4705 - val_loss: 3.0277 - val_acc: 0.4766\n",
            "Epoch 41/41\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.0094 - acc: 0.4732 - val_loss: 3.0352 - val_acc: 0.4844\n",
            "Epoch 42/42\n",
            "390/390 [==============================] - 63s 163ms/step - loss: 3.0044 - acc: 0.4766 - val_loss: 2.9597 - val_acc: 0.5014\n",
            "Epoch 43/43\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.0212 - acc: 0.4779 - val_loss: 2.8864 - val_acc: 0.5139\n",
            "Epoch 44/44\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.0112 - acc: 0.4837 - val_loss: 2.9800 - val_acc: 0.5044\n",
            "Epoch 45/45\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.0215 - acc: 0.4845 - val_loss: 2.9281 - val_acc: 0.5185\n",
            "Epoch 46/46\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.0168 - acc: 0.4903 - val_loss: 2.8627 - val_acc: 0.5295\n",
            "Epoch 47/47\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.0050 - acc: 0.4940 - val_loss: 3.0102 - val_acc: 0.5035\n",
            "Epoch 48/48\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 3.0199 - acc: 0.4964 - val_loss: 2.8743 - val_acc: 0.5324\n",
            "Epoch 49/49\n",
            "390/390 [==============================] - 65s 166ms/step - loss: 3.0122 - acc: 0.4959 - val_loss: 2.9218 - val_acc: 0.5260\n",
            "Epoch 50/50\n",
            "390/390 [==============================] - 71s 183ms/step - loss: 2.7666 - acc: 0.5437 - val_loss: 2.6802 - val_acc: 0.5599\n",
            "Epoch 51/51\n",
            "390/390 [==============================] - 64s 164ms/step - loss: 2.6303 - acc: 0.5584 - val_loss: 2.6799 - val_acc: 0.5490\n",
            "Epoch 52/52\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 2.5820 - acc: 0.5575 - val_loss: 2.5020 - val_acc: 0.5719\n",
            "Epoch 53/53\n",
            "390/390 [==============================] - 64s 164ms/step - loss: 2.5380 - acc: 0.5620 - val_loss: 2.6041 - val_acc: 0.5586\n",
            "Epoch 54/54\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 2.5332 - acc: 0.5596 - val_loss: 2.5597 - val_acc: 0.5651\n",
            "Epoch 55/55\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 2.5177 - acc: 0.5671 - val_loss: 2.6992 - val_acc: 0.5381\n",
            "Epoch 56/56\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 2.5119 - acc: 0.5671 - val_loss: 2.5365 - val_acc: 0.5713\n",
            "Epoch 57/57\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 2.5093 - acc: 0.5660 - val_loss: 2.5713 - val_acc: 0.5637\n",
            "Epoch 58/58\n",
            "390/390 [==============================] - 64s 164ms/step - loss: 2.5149 - acc: 0.5694 - val_loss: 2.5065 - val_acc: 0.5759\n",
            "Epoch 59/59\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 2.5071 - acc: 0.5694 - val_loss: 2.6314 - val_acc: 0.5575\n",
            "Epoch 60/60\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 2.5091 - acc: 0.5728 - val_loss: 2.5992 - val_acc: 0.5631\n",
            "Epoch 61/61\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 2.5213 - acc: 0.5716 - val_loss: 2.6177 - val_acc: 0.5581\n",
            "Epoch 62/62\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 2.5161 - acc: 0.5726 - val_loss: 2.5767 - val_acc: 0.5749\n",
            "Epoch 63/63\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 2.5101 - acc: 0.5788 - val_loss: 2.5071 - val_acc: 0.5830\n",
            "Epoch 64/64\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 2.5138 - acc: 0.5808 - val_loss: 2.5741 - val_acc: 0.5750\n",
            "Epoch 65/65\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 2.5187 - acc: 0.5797 - val_loss: 2.5628 - val_acc: 0.5755\n",
            "Epoch 66/66\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 2.5115 - acc: 0.5803 - val_loss: 2.5253 - val_acc: 0.5910\n",
            "Epoch 67/67\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 2.5212 - acc: 0.5829 - val_loss: 2.6426 - val_acc: 0.5625\n",
            "Epoch 68/68\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 2.5244 - acc: 0.5828 - val_loss: 2.6438 - val_acc: 0.5647\n",
            "Epoch 69/69\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 2.5261 - acc: 0.5816 - val_loss: 2.5374 - val_acc: 0.5827\n",
            "Epoch 70/70\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 2.5257 - acc: 0.5858 - val_loss: 2.5830 - val_acc: 0.5815\n",
            "Epoch 71/71\n",
            "390/390 [==============================] - 64s 164ms/step - loss: 2.5269 - acc: 0.5871 - val_loss: 2.5844 - val_acc: 0.5819\n",
            "Epoch 72/72\n",
            "390/390 [==============================] - 64s 164ms/step - loss: 2.5199 - acc: 0.5902 - val_loss: 2.5919 - val_acc: 0.5806\n",
            "Epoch 73/73\n",
            "390/390 [==============================] - 63s 163ms/step - loss: 2.5323 - acc: 0.5884 - val_loss: 2.5707 - val_acc: 0.5898\n",
            "Epoch 74/74\n",
            "390/390 [==============================] - 63s 163ms/step - loss: 2.5325 - acc: 0.5895 - val_loss: 2.7193 - val_acc: 0.5632\n",
            "Epoch 75/75\n",
            "390/390 [==============================] - 71s 182ms/step - loss: 2.3259 - acc: 0.6380 - val_loss: 2.4119 - val_acc: 0.6163\n",
            "Epoch 76/76\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 2.2436 - acc: 0.6474 - val_loss: 2.4454 - val_acc: 0.6049\n",
            "Epoch 77/77\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 2.1875 - acc: 0.6525 - val_loss: 2.3124 - val_acc: 0.6261\n",
            "Epoch 78/78\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.1670 - acc: 0.6520 - val_loss: 2.3196 - val_acc: 0.6221\n",
            "Epoch 79/79\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.1383 - acc: 0.6527 - val_loss: 2.2864 - val_acc: 0.6232\n",
            "Epoch 80/80\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.1262 - acc: 0.6511 - val_loss: 2.3174 - val_acc: 0.6213\n",
            "Epoch 81/81\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.1112 - acc: 0.6540 - val_loss: 2.3437 - val_acc: 0.6131\n",
            "Epoch 82/82\n",
            "390/390 [==============================] - 62s 159ms/step - loss: 2.0929 - acc: 0.6573 - val_loss: 2.2868 - val_acc: 0.6244\n",
            "Epoch 83/83\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.0994 - acc: 0.6513 - val_loss: 2.2995 - val_acc: 0.6203\n",
            "Epoch 84/84\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.0795 - acc: 0.6568 - val_loss: 2.3075 - val_acc: 0.6155\n",
            "Epoch 85/85\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.0821 - acc: 0.6562 - val_loss: 2.2381 - val_acc: 0.6242\n",
            "Epoch 86/86\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.0866 - acc: 0.6546 - val_loss: 2.3211 - val_acc: 0.6094\n",
            "Epoch 87/87\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.0785 - acc: 0.6583 - val_loss: 2.2347 - val_acc: 0.6354\n",
            "Epoch 88/88\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.0832 - acc: 0.6561 - val_loss: 2.2987 - val_acc: 0.6205\n",
            "Epoch 89/89\n",
            "390/390 [==============================] - 63s 160ms/step - loss: 2.0694 - acc: 0.6615 - val_loss: 2.3034 - val_acc: 0.6214\n",
            "Epoch 90/90\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.0778 - acc: 0.6591 - val_loss: 2.2805 - val_acc: 0.6282\n",
            "Epoch 91/91\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.0784 - acc: 0.6604 - val_loss: 2.3043 - val_acc: 0.6228\n",
            "Epoch 92/92\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.0824 - acc: 0.6598 - val_loss: 2.2701 - val_acc: 0.6292\n",
            "Epoch 93/93\n",
            "390/390 [==============================] - 62s 159ms/step - loss: 2.0775 - acc: 0.6601 - val_loss: 2.2979 - val_acc: 0.6243\n",
            "Epoch 94/94\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.0803 - acc: 0.6591 - val_loss: 2.2396 - val_acc: 0.6409\n",
            "Epoch 95/95\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.0760 - acc: 0.6636 - val_loss: 2.3342 - val_acc: 0.6231\n",
            "Epoch 96/96\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.0710 - acc: 0.6665 - val_loss: 2.3864 - val_acc: 0.6109\n",
            "Epoch 97/97\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.0849 - acc: 0.6631 - val_loss: 2.2671 - val_acc: 0.6341\n",
            "Epoch 98/98\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.0773 - acc: 0.6657 - val_loss: 2.3539 - val_acc: 0.6189\n",
            "Epoch 99/99\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 2.0703 - acc: 0.6666 - val_loss: 2.2530 - val_acc: 0.6389\n",
            "Epoch 100/100\n",
            "390/390 [==============================] - 70s 179ms/step - loss: 1.9290 - acc: 0.7041 - val_loss: 2.2065 - val_acc: 0.6506\n",
            "Epoch 101/101\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.8557 - acc: 0.7152 - val_loss: 2.2285 - val_acc: 0.6480\n",
            "Epoch 102/102\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.8363 - acc: 0.7170 - val_loss: 2.2840 - val_acc: 0.6348\n",
            "Epoch 103/103\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.8097 - acc: 0.7184 - val_loss: 2.1613 - val_acc: 0.6529\n",
            "Epoch 104/104\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.7780 - acc: 0.7228 - val_loss: 2.1515 - val_acc: 0.6537\n",
            "Epoch 105/105\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.7693 - acc: 0.7240 - val_loss: 2.1528 - val_acc: 0.6541\n",
            "Epoch 106/106\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.7605 - acc: 0.7249 - val_loss: 2.1787 - val_acc: 0.6481\n",
            "Epoch 107/107\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.7444 - acc: 0.7238 - val_loss: 2.2564 - val_acc: 0.6401\n",
            "Epoch 108/108\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.7375 - acc: 0.7242 - val_loss: 2.1512 - val_acc: 0.6458\n",
            "Epoch 109/109\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.7245 - acc: 0.7288 - val_loss: 2.1138 - val_acc: 0.6556\n",
            "Epoch 110/110\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.7172 - acc: 0.7284 - val_loss: 2.1366 - val_acc: 0.6543\n",
            "Epoch 111/111\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.7148 - acc: 0.7270 - val_loss: 2.1323 - val_acc: 0.6521\n",
            "Epoch 112/112\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.7099 - acc: 0.7268 - val_loss: 2.1289 - val_acc: 0.6504\n",
            "Epoch 113/113\n",
            "390/390 [==============================] - 63s 160ms/step - loss: 1.7009 - acc: 0.7286 - val_loss: 2.1319 - val_acc: 0.6513\n",
            "Epoch 114/114\n",
            "390/390 [==============================] - 63s 160ms/step - loss: 1.7050 - acc: 0.7270 - val_loss: 2.1380 - val_acc: 0.6510\n",
            "Epoch 115/115\n",
            "390/390 [==============================] - 63s 160ms/step - loss: 1.7016 - acc: 0.7250 - val_loss: 2.1344 - val_acc: 0.6507\n",
            "Epoch 116/116\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.7028 - acc: 0.7268 - val_loss: 2.0695 - val_acc: 0.6609\n",
            "Epoch 117/117\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.6905 - acc: 0.7308 - val_loss: 2.1002 - val_acc: 0.6607\n",
            "Epoch 118/118\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.6850 - acc: 0.7308 - val_loss: 2.1260 - val_acc: 0.6518\n",
            "Epoch 119/119\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.6929 - acc: 0.7274 - val_loss: 2.2376 - val_acc: 0.6307\n",
            "Epoch 120/120\n",
            "390/390 [==============================] - 63s 160ms/step - loss: 1.6954 - acc: 0.7310 - val_loss: 2.2190 - val_acc: 0.6364\n",
            "Epoch 121/121\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.6872 - acc: 0.7290 - val_loss: 2.0804 - val_acc: 0.6591\n",
            "Epoch 122/122\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.6873 - acc: 0.7312 - val_loss: 2.0106 - val_acc: 0.6750\n",
            "Epoch 123/123\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.6865 - acc: 0.7331 - val_loss: 2.1276 - val_acc: 0.6537\n",
            "Epoch 124/124\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.6743 - acc: 0.7348 - val_loss: 2.1751 - val_acc: 0.6462\n",
            "Epoch 125/125\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.5656 - acc: 0.7639Epoch 126/126\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.5253 - acc: 0.7704 - val_loss: 2.0578 - val_acc: 0.6757\n",
            "Epoch 127/127\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.4998 - acc: 0.7754 - val_loss: 2.0560 - val_acc: 0.6754\n",
            "Epoch 128/128\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.4832 - acc: 0.7773 - val_loss: 2.0355 - val_acc: 0.6758\n",
            "Epoch 129/129\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.4695 - acc: 0.7789 - val_loss: 2.0641 - val_acc: 0.6692\n",
            "Epoch 130/130\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.4531 - acc: 0.7813 - val_loss: 2.0887 - val_acc: 0.6623\n",
            "Epoch 131/131\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.4397 - acc: 0.7844 - val_loss: 2.1278 - val_acc: 0.6536\n",
            "Epoch 132/132\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.4376 - acc: 0.7847 - val_loss: 2.0135 - val_acc: 0.6800\n",
            "Epoch 133/133\n",
            "390/390 [==============================] - 63s 160ms/step - loss: 1.4210 - acc: 0.7858 - val_loss: 2.0629 - val_acc: 0.6695\n",
            "Epoch 134/134\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.4136 - acc: 0.7873 - val_loss: 2.0809 - val_acc: 0.6676\n",
            "Epoch 135/135\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.4027 - acc: 0.7864 - val_loss: 2.0517 - val_acc: 0.6710\n",
            "Epoch 136/136\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.3993 - acc: 0.7868 - val_loss: 1.9777 - val_acc: 0.6813\n",
            "Epoch 137/137\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.3966 - acc: 0.7875 - val_loss: 1.9592 - val_acc: 0.6832\n",
            "Epoch 138/138\n",
            "390/390 [==============================] - 63s 160ms/step - loss: 1.3884 - acc: 0.7856 - val_loss: 2.0655 - val_acc: 0.6636\n",
            "Epoch 139/139\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 1.3908 - acc: 0.7860 - val_loss: 2.0428 - val_acc: 0.6717\n",
            "Epoch 140/140\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.3830 - acc: 0.7885 - val_loss: 2.0043 - val_acc: 0.6746\n",
            "Epoch 141/141\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.3726 - acc: 0.7895 - val_loss: 2.0115 - val_acc: 0.6701\n",
            "Epoch 142/142\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.3666 - acc: 0.7912 - val_loss: 1.9601 - val_acc: 0.6829\n",
            "Epoch 143/143\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.3639 - acc: 0.7909 - val_loss: 2.0376 - val_acc: 0.6647\n",
            "Epoch 144/144\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.3595 - acc: 0.7905 - val_loss: 1.9685 - val_acc: 0.6833\n",
            "Epoch 145/145\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.3606 - acc: 0.7909 - val_loss: 2.0340 - val_acc: 0.6658\n",
            "Epoch 146/146\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.3498 - acc: 0.7941 - val_loss: 2.0077 - val_acc: 0.6746\n",
            "Epoch 147/147\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.3528 - acc: 0.7934 - val_loss: 2.0467 - val_acc: 0.6658\n",
            "Epoch 148/148\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.3511 - acc: 0.7913 - val_loss: 2.0662 - val_acc: 0.6589\n",
            "Epoch 149/149\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.3420 - acc: 0.7937 - val_loss: 1.9493 - val_acc: 0.6801\n",
            "Accuracy: 68.01%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAASO0lEQVR4nO3dfZBddX3H8fcXsmaJwTztgoZUEttK\nw0MNsiCMzDSISAJKQSwjD9XSOnFa26FVGUIhIOofPIwWKQOZyGRihzFIQaQKdgI2MUzFMgsGWSSQ\n8CRLlCxJQR4SSuDbP/YGFtzdu7v37N7sL+/XzJ099/6+55zvLzvz4XDOuWcjM5EkjX97NLsBSVI1\nDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6NptRMQTEfHRZvchjRYDXZIKYaBLUiEMdO12ImJiRFwZ\nEZtqrysjYmJtrC0ifhQRz0XE1oi4KyL2qI2dFxFPR8QLEfFwRBzb3JlIbzWh2Q1ITXABcCQwD0jg\nVuBCYAnwJaAbaK/VHglkRBwA/D1weGZuiojZwJ5j27Y0OI/QtTs6E/hqZm7OzB7gEuAva2OvAu8B\n9s/MVzPzrux94NFrwETgwIhoycwnMvPRpnQvDcBA1+5oJvBkn/dP1j4DuALYCKyKiMciYjFAZm4E\n/hH4CrA5Im6IiJlIuxADXbujTcD+fd6/t/YZmflCZn4pM98HnAR8cee58sz8bmYeXVs3gcvGtm1p\ncAa6dkcrgQsjoj0i2oCLgOsBIuLjEfFHERHA8/Seank9Ig6IiI/ULp5uB7YBrzepf6lfBrp2R18H\nOoFfAg8A99U+A/hj4E7gReBu4JrMXE3v+fNLgWeB3wL7AOePbdvS4MI/cCFJZfAIXZIKYaBLUiEM\ndEkqhIEuSYVo2lf/29racvbs2c3avSSNS/fee++zmdne31jTAn327Nl0dnY2a/eSNC5FxJMDjXnK\nRZIKYaBLUiEMdEkqhM9DlzSuvPrqq3R3d7N9+/ZmtzKqWltbmTVrFi0tLUNex0CXNK50d3ez9957\nM3v2bHqfoVaezGTLli10d3czZ86cIa/nKRdJ48r27duZMWNGsWEOEBHMmDFj2P8XYqBLGndKDvOd\nRjJHA12SCmGgS9IwPPfcc1xzzTXDXu+EE07gueeeG4WO3mSgS9IwDBToO3bsGHS922+/nalTp45W\nW4B3uUjSsCxevJhHH32UefPm0dLSQmtrK9OmTWP9+vU88sgjnHzyyTz11FNs376dc845h0WLFgFv\nPu7kxRdfZOHChRx99NH87Gc/Y7/99uPWW29lr732arg3A13SuHXJDx/kV5t+V+k2D5z5Li7+xEED\njl966aV0dXWxbt061qxZw4knnkhXV9cbtxcuX76c6dOns23bNg4//HBOPfVUZsyY8ZZtbNiwgZUr\nV/Ltb3+b0047jZtvvpmzzjqr4d4NdElqwBFHHPGWe8WvuuoqbrnlFgCeeuopNmzY8HuBPmfOHObN\nmwfAYYcdxhNPPFFJLwa6pHFrsCPpsfLOd77zjeU1a9Zw5513cvfddzNp0iTmz5/f773kEydOfGN5\nzz33ZNu2bZX04kVRSRqGvffemxdeeKHfseeff55p06YxadIk1q9fz89//vMx7c0jdEkahhkzZvDh\nD3+Ygw8+mL322ot99933jbEFCxawdOlS5s6dywEHHMCRRx45pr1FZo7pDnfq6OhI/8CFpOF66KGH\nmDt3brPbGBP9zTUi7s3Mjv7qPeUiSYUw0CWpEAa6JBXCQJekQtQN9IhYHhGbI6JrgPFpEXFLRPwy\nIu6JiIOrb1OSVM9QjtBXAAsGGf9nYF1m/inwGeBbFfQlSRqmuoGemWuBrYOUHAj8V612PTA7IvYd\npF6Sxq2RPj4X4Morr+Tll1+uuKM3VXEO/X7gkwARcQSwPzCrv8KIWBQRnRHR2dPTU8GuJWls7cqB\nXsU3RS8FvhUR64AHgF8Ar/VXmJnLgGXQ+8WiCvYtSWOq7+NzjzvuOPbZZx9uvPFGXnnlFU455RQu\nueQSXnrpJU477TS6u7t57bXXWLJkCc888wybNm3imGOOoa2tjdWrV1feW8OBnpm/A84GiN4/gvc4\n8Fij25Wkun68GH77QLXbfPchsPDSAYf7Pj531apV3HTTTdxzzz1kJieddBJr166lp6eHmTNncttt\ntwG9z3iZMmUK3/zmN1m9ejVtbW3V9lzT8CmXiJgaEe+ovf0csLYW8pJUtFWrVrFq1SoOPfRQPvjB\nD7J+/Xo2bNjAIYccwh133MF5553HXXfdxZQpU8akn7pH6BGxEpgPtEVEN3Ax0AKQmUuBucB3IiKB\nB4G/GbVuJamvQY6kx0Jmcv755/P5z3/+98buu+8+br/9di688EKOPfZYLrroolHvp26gZ+bpdcbv\nBt5fWUeStAvr+/jc448/niVLlnDmmWcyefJknn76aVpaWtixYwfTp0/nrLPOYurUqVx33XVvWXe0\nTrn4+FxJGoa+j89duHAhZ5xxBkcddRQAkydP5vrrr2fjxo2ce+657LHHHrS0tHDttdcCsGjRIhYs\nWMDMmTNH5aKoj8+VNK74+FwfnytJxTPQJakQBrqkcadZp4rH0kjmaKBLGldaW1vZsmVL0aGemWzZ\nsoXW1tZhreddLpLGlVmzZtHd3U3pz4NqbW1l1qx+H4s1IANd0rjS0tLCnDlzmt3GLslTLpJUCANd\nkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWp\nEAa6JBXCQJekQtQN9IhYHhGbI6JrgPEpEfHDiLg/Ih6MiLOrb1OSVM9QjtBXAAsGGf8C8KvM/AAw\nH/hGRLyj8dYkScNRN9Azcy2wdbASYO+ICGByrXZHNe1JkoaqinPoVwNzgU3AA8A5mfl6f4URsSgi\nOiOis/Q/8CpJY62KQD8eWAfMBOYBV0fEu/orzMxlmdmRmR3t7e0V7FqStFMVgX428P3stRF4HPiT\nCrYrSRqGKgL918CxABGxL3AA8FgF25UkDcOEegURsZLeu1faIqIbuBhoAcjMpcDXgBUR8QAQwHmZ\n+eyodSxJ6lfdQM/M0+uMbwI+VllHkqQR8ZuiklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQV\nwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEM\ndEkqhIEuSYUw0CWpEAa6JBWibqBHxPKI2BwRXQOMnxsR62qvroh4LSKmV9+qJGkwQzlCXwEsGGgw\nM6/IzHmZOQ84H/hpZm6tqD9J0hDVDfTMXAsMNaBPB1Y21JEkaUQqO4ceEZPoPZK/eZCaRRHRGRGd\nPT09Ve1akkS1F0U/Afz3YKdbMnNZZnZkZkd7e3uFu5YkVRnon8bTLZLUNJUEekRMAf4MuLWK7UmS\nhm9CvYKIWAnMB9oiohu4GGgByMyltbJTgFWZ+dIo9SlJqqNuoGfm6UOoWUHv7Y2SpCbxm6KSVAgD\nXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAl\nqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RC1A30iFgeEZsj\nomuQmvkRsS4iHoyIn1bboiRpKIZyhL4CWDDQYERMBa4BTsrMg4C/qKY1SdJw1A30zFwLbB2k5Azg\n+5n561r95op6kyQNQxXn0N8PTIuINRFxb0R8ZqDCiFgUEZ0R0dnT01PBriVJO1UR6BOAw4ATgeOB\nJRHx/v4KM3NZZnZkZkd7e3sFu5Yk7TShgm10A1sy8yXgpYhYC3wAeKSCbUuShqiKI/RbgaMjYkJE\nTAI+BDxUwXYlScNQ9wg9IlYC84G2iOgGLgZaADJzaWY+FBH/CfwSeB24LjMHvMVRkjQ66gZ6Zp4+\nhJorgCsq6UiSNCJ+U1SSCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtS\nIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXC\nQJekQtQN9IhYHhGbI6JrgPH5EfF8RKyrvS6qvk1JUj0ThlCzArga+LdBau7KzI9X0pEkaUTqHqFn\n5lpg6xj0IklqQFXn0I+KiPsj4scRcdBARRGxKCI6I6Kzp6enol1LkqCaQL8P2D8zPwD8K/CDgQoz\nc1lmdmRmR3t7ewW7liTt1HCgZ+bvMvPF2vLtQEtEtDXcmSRpWBoO9Ih4d0REbfmI2ja3NLpdSdLw\n1L3LJSJWAvOBtojoBi4GWgAycynwKeBvI2IHsA34dGbmqHUsSepX3UDPzNPrjF9N722NkqQm8pui\nklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5J\nhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQtQN9IhY\nHhGbI6KrTt3hEbEjIj5VXXuSpKEayhH6CmDBYAURsSdwGbCqgp4kSSNQN9Azcy2wtU7ZPwA3A5ur\naEqSNHwNn0OPiP2AU4Brh1C7KCI6I6Kzp6en0V1Lkvqo4qLolcB5mfl6vcLMXJaZHZnZ0d7eXsGu\nJUk7TahgGx3ADREB0AacEBE7MvMHFWxbkjREDQd6Zs7ZuRwRK4AfGeaSNPbqBnpErATmA20R0Q1c\nDLQAZObSUe1OkjRkdQM9M08f6sYy868a6kaSNGJ+U1SSCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQV\nwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEM\ndEkqhIEuSYUw0CWpEAa6JBXCQJekQkRmNmfHET3Ak03ZeWPagGeb3cQYc87l293mC+N3zvtnZnt/\nA00L9PEqIjozs6PZfYwl51y+3W2+UOacPeUiSYUw0CWpEAb68C1rdgNN4JzLt7vNFwqcs+fQJakQ\nHqFLUiEMdEkqhIHej4iYHhF3RMSG2s9pA9R9tlazISI+28/4f0RE1+h33LhG5hwRkyLitohYHxEP\nRsSlY9v90EXEgoh4OCI2RsTifsYnRsT3auP/ExGz+4ydX/v84Yg4fiz7bsRI5xwRx0XEvRHxQO3n\nR8a695Fq5PdcG39vRLwYEV8eq54rkZm+3vYCLgcW15YXA5f1UzMdeKz2c1pteVqf8U8C3wW6mj2f\n0Z4zMAk4plbzDuAuYGGz59RP/3sCjwLvq/V5P3Dg22r+DlhaW/408L3a8oG1+onAnNp29mz2nEZ5\nzocCM2vLBwNPN3s+oz3nPuM3Af8OfLnZ8xnOyyP0/v058J3a8neAk/upOR64IzO3Zub/AncACwAi\nYjLwReDrY9BrVUY858x8OTNXA2Tm/wH3AbPGoOfhOgLYmJmP1fq8gd5599X33+Em4NiIiNrnN2Tm\nK5n5OLCxtr1d3YjnnJm/yMxNtc8fBPaKiIlj0nVjGvk9ExEnA4/TO+dxxUDv376Z+Zva8m+Bffup\n2Q94qs/77tpnAF8DvgG8PGodVq/ROQMQEVOBTwA/GY0mG1S3/741mbkDeB6YMcR1d0WNzLmvU4H7\nMvOVUeqzSiOec+1g7DzgkjHos3ITmt1As0TEncC7+xm6oO+bzMyIGPK9nRExD/jDzPynt5+Xa7bR\nmnOf7U8AVgJXZeZjI+tSu5qIOAi4DPhYs3sZA18B/iUzX6wdsI8ru22gZ+ZHBxqLiGci4j2Z+ZuI\neA+wuZ+yp4H5fd7PAtYARwEdEfEEvf+++0TEmsycT5ON4px3WgZsyMwrK2h3NDwN/EGf97Nqn/VX\n0137D9QUYMsQ190VNTJnImIWcAvwmcx8dPTbrUQjc/4Q8KmIuByYCrweEdsz8+rRb7sCzT6Jvyu+\ngCt46wXCy/upmU7vebZptdfjwPS31cxm/FwUbWjO9F4vuBnYo9lzGWSOE+i9kDuHNy+WHfS2mi/w\n1otlN9aWD+KtF0UfY3xcFG1kzlNr9Z9s9jzGas5vq/kK4+yiaNMb2BVf9J4//AmwAbizT2h1ANf1\nqftrei+ObQTO7mc74ynQRzxneo+AEngIWFd7fa7ZcxpgnicAj9B7F8QFtc++CpxUW26l9+6GjcA9\nwPv6rHtBbb2H2QXv4ql6zsCFwEt9fqfrgH2aPZ/R/j332ca4C3S/+i9JhfAuF0kqhIEuSYUw0CWp\nEAa6JBXCQJekQhjoklQIA12SCvH/5oy/pJnEmXYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}